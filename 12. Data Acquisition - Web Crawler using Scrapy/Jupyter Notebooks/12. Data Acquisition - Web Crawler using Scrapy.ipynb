{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Scrapy - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy\n",
    "- An open source and collaborative framework for extracting the data you need from websites\n",
    "\n",
    "### Installation\n",
    "- pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in /home/dandy/Keep/Data-Science-Master-Course/12. Data Acquisition - Web Crawler using Scrapy/Jupyter Notebooks/myproject\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject myproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Creating our First Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scrapy.org/en/latest/intro/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a project\n",
    "Before you start scraping, you will have to set up a new Scrapy project. Enter a directory where you’d like to store your code and run:\n",
    "\n",
    "scrapy startproject tutorial\n",
    "\n",
    "This will create a tutorial directory with the following contents:\n",
    "\n",
    "tutorial/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "\n",
    "    tutorial/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # project items definition file\n",
    "\n",
    "        middlewares.py    # project middlewares file\n",
    "\n",
    "        pipelines.py      # project pipelines file\n",
    "\n",
    "        settings.py       # project settings file\n",
    "\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first Spider\n",
    "Spiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass scrapy.Spider and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.\n",
    "\n",
    "This is the code for our first Spider. Save it in a file named quotes_spider.py under the tutorial/spiders directory in your project:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)\n",
    "As you can see, our Spider subclasses scrapy.Spider and defines some attributes and methods:\n",
    "\n",
    "name: identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n",
    "\n",
    "start_requests(): must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n",
    "\n",
    "parse(): a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of TextResponse that holds the page content and has further helpful methods to handle it.\n",
    "\n",
    "The parse() method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (Request) from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../Images/Image1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Using Shell and Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../Images/Image2.png'/>\n",
    "<img src='../Images/Image3.png'/>\n",
    "<img src='../Images/Image4.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Parsing Response as JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../Images/Image5.png'/>\n",
    "<img src='../Images/Image6.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Recursive Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../Images/Image7.png'/>\n",
    "<img src='../Images/Image8.png'/>\n",
    "<img src='../Images/Image9.png'/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
